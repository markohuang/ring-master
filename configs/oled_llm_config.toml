[setupparams]
name='oled_llm'
vocab_path='sorted_vocab.txt'
smiles_path='good_smiles.txt'
dataset_key='text'
max_cand_size=40 # arbitrary
max_atom_neighbors=5 # arbitrary
max_motif_neighbors=20 # arbitrary
max_length=100 # arbitrary


[trainingparams]
smiles_max_length=120 # arbitrary
clip_dim=64
latent_size=48
cands_hidden_size=24
hidden_size=64
batch_size=128
n_positions=102 # +2 for bos and eos
n_smiles_positions=121 # +1 for bos
# training setup
warmup_steps=10000
smiles_vocab_size=290

# GPT2Config
n_layer=6
n_head=6
vocab_size=10015
n_embd=384
add_cross_attention=true
lr=1e-5 # 5e-05


# for pl.Trainer
max_epochs=100
accelerator="auto"
# val_check_interval=2000
check_val_every_n_epoch=1
gradient_clip_val=1.0
accumulate_grad_batches=10
# enable_progress_bar=true
devices=1
# log_every_n_steps=1